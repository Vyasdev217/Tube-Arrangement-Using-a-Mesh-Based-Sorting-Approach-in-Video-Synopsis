{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tube Arrangement Using a Mesh-Based Sorting Approach in Video Synopsis\n",
    "DOI: 10.1109/LSP.2025.3631432\n",
    "\n",
    "https://ieeexplore.ieee.org/document/11242038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display system status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nD3EEfKzFftV",
    "outputId": "f905d6e9-f1d4-480a-b624-3d46d5421d75",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!free -h\n",
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "AO4MWPmnFk7m",
    "outputId": "c9407c36-f4d0-4e19-b604-f7df35e14baa",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --cache-dir=pip_cache tensorflow\n",
    "!pip install --cache-dir=pip_cache ultralytics\n",
    "!pip install --cache-dir=pip_cache supervision\n",
    "!pip install --cache-dir=pip_cache python-opencv\n",
    "!pip install --cache-dir=pip_cache scikit-image\n",
    "!pip install --cache-dir=pip_cache trimesh\n",
    "!pip install --cache-dir=pip_cache matplotlib\n",
    "!pip install --cache-dir=pip_cache moviepy\n",
    "!pip install --cache-dir=pip_cache numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from gc import collect as garbage_collect\n",
    "from skimage.measure import marching_cubes\n",
    "import trimesh\n",
    "from ultralytics import YOLO\n",
    "from moviepy import ImageSequenceClip\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load input video details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd\n",
    "HOME = getcwd()\n",
    "INPUT_VIDEO_PATH = f\"{HOME}/TownCentreXVID-mini-nowatermark.mp4\"\n",
    "#INPUT_VIDEO_PATH = f\"{HOME}/inputs/30s.mp4\"\n",
    "OUTPUT_VIDEO_PATH = f\"{HOME}/output.mp4\"\n",
    "\n",
    "import supervision as sv\n",
    "video_info = sv.VideoInfo.from_video_path(INPUT_VIDEO_PATH)\n",
    "print(video_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO11 segmentation and tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the width and height of output tensor for YOLO11 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdnkBZVn9Xyb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_stride = 32\n",
    "video_info.height = ((video_info.height + max_stride - 1) // max_stride) * max_stride\n",
    "video_info.width = ((video_info.width + max_stride - 1) // max_stride) * max_stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run YOLO11 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "noOycxILHjYI",
    "outputId": "ce1a65d4-ad00-4de0-acf8-dca161f5d6c5"
   },
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11n-seg.pt\")\n",
    "model.fuse()\n",
    "seg_track_results = model.track(INPUT_VIDEO_PATH, conf=0.5, iou=0.5, imgsz=(video_info.height, video_info.width), show=False, stream=True, verbose=False, classes=[0,24,26,28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine YOLO11 outputs to a single array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segmented_frames_arr = list()\n",
    "object_ids = set()\n",
    "object_tube_box = defaultdict(lambda: {\"frame\":{\"start\":np.inf, \"end\":0},\"x\":{\"start\":np.inf, \"end\":0},\"y\":{\"start\":np.inf, \"end\":0}})\n",
    "frame_num = 0\n",
    "\n",
    "timestamp_dict = defaultdict(lambda: list())\n",
    "for res_frame in tqdm(seg_track_results, desc = \"Object Segmentation and Tracking\"):\n",
    "    if res_frame.boxes.id == None:\n",
    "        segmented_frames_arr.append(tf.zeros([video_info.height, video_info.width]))\n",
    "        frame_num += 1\n",
    "        continue\n",
    "    frame = res_frame.masks.data[0]*res_frame.boxes.id[0]\n",
    "    for i in range(1, len(res_frame.boxes.id)):\n",
    "        oid = int(res_frame.boxes.id[i])\n",
    "        if oid not in object_ids:\n",
    "            object_ids.add(oid)\n",
    "            object_tube_box[oid]['frame']['start'] = frame_num\n",
    "        object_tube_box[oid]['frame']['end'] = frame_num\n",
    "        object_tube_box[oid]['x']['start'] = min(object_tube_box[oid]['x']['start'], int(res_frame.boxes.xyxy[i][0]))\n",
    "        object_tube_box[oid]['x']['end'] = max(object_tube_box[oid]['x']['end'], int(res_frame.boxes.xyxy[i][2]))\n",
    "        object_tube_box[oid]['y']['start'] = min(object_tube_box[oid]['y']['start'], int(res_frame.boxes.xyxy[i][1]))\n",
    "        object_tube_box[oid]['y']['end'] = max(object_tube_box[oid]['y']['end'], int(res_frame.boxes.xyxy[i][3]))\n",
    "        frame = frame + res_frame.masks.data[i]*res_frame.boxes.id[i]\n",
    "        timestamp_dict[oid].append((frame_num + 1, (res_frame.boxes.xyxy[i][0] + res_frame.boxes.xyxy[i][2]) // 2, int(res_frame.boxes.xyxy[i][1])))\n",
    "    segmented_frames_arr.append(frame.cpu())\n",
    "    frame_num += 1\n",
    "del model, seg_track_results\n",
    "segmented_frames_arr = tf.stack(segmented_frames_arr, axis = 0)\n",
    "segmented_frames_arr = tf.cast(segmented_frames_arr, tf.int32)\n",
    "segmented_frames_arr = segmented_frames_arr.cpu().numpy()\n",
    "tf.keras.backend.clear_session()\n",
    "garbage_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(object_tensor):\n",
    "    centroids = {}\n",
    "    for f in range(object_tensor.shape[0]):\n",
    "        centroids[f] = {}\n",
    "        ids = np.unique(object_tensor[f])\n",
    "        for obj_id in ids:\n",
    "            if obj_id == 0:\n",
    "                continue\n",
    "            y, x = np.where(object_tensor[f] == obj_id)\n",
    "            if len(x) > 0:\n",
    "                cx, cy = np.mean(x), np.mean(y)\n",
    "                centroids[f][obj_id] = (cx, cy)\n",
    "    return centroids\n",
    "\n",
    "def detect_interactions_from_tensor(object_tensor, fps, height, proximity_thresh=60):\n",
    "    centroids = compute_centroids(object_tensor)\n",
    "    interaction_frames = defaultdict(lambda: defaultdict(list))\n",
    "    num_frames = object_tensor.shape[0]\n",
    "    for f in range(num_frames):\n",
    "        frame_data = centroids[f]\n",
    "        ids = list(frame_data.keys())\n",
    "        for i in range(len(ids)):\n",
    "            for j in range(i + 1, len(ids)):\n",
    "                id1, id2 = ids[i], ids[j]\n",
    "                c1, c2 = frame_data[id1], frame_data[id2]\n",
    "                dist = np.linalg.norm(np.array(c1) - np.array(c2))\n",
    "                ht = height/20                                           #need to take the y-length of possible objects\n",
    "                if dist < proximity_thresh:\n",
    "                    interaction_frames[min(id1, id2)][max(id1, id2)].append(f)\n",
    "    interactions = []\n",
    "    min_duration = int(2 * fps)\n",
    "    for id1 in interaction_frames:\n",
    "        for id2 in interaction_frames[id1]:\n",
    "            frames = interaction_frames[id1][id2]\n",
    "            if not frames:\n",
    "                continue\n",
    "            grouped = []\n",
    "            temp = [frames[0]]\n",
    "            for idx in range(1, len(frames)):\n",
    "                if frames[idx] == frames[idx - 1] + 1:\n",
    "                    temp.append(frames[idx])\n",
    "                else:\n",
    "                    if len(temp) >= min_duration:\n",
    "                        grouped.append(temp)\n",
    "                    temp = [frames[idx]]\n",
    "            if len(temp) >= min_duration:\n",
    "                grouped.append(temp)\n",
    "            for grp in grouped:\n",
    "                start_time = grp[0]\n",
    "                end_time = grp[-1]\n",
    "                interactions.append({\n",
    "                    \"object_id\": id1,\n",
    "                    \"interacting_with\": id2,\n",
    "                    \"start_time\": round(start_time, 2),\n",
    "                    \"end_time\": round(end_time, 2)\n",
    "                })\n",
    "    return interactions\n",
    "interactions0 = detect_interactions_from_tensor(segmented_frames_arr, video_info.fps, video_info.height)\n",
    "interactions = [(i[\"object_id\"], i[\"interacting_with\"]) for i in interactions0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Object Tubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping of objects according to detected interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_groups(pairs):\n",
    "    from collections import defaultdict\n",
    "    graph = defaultdict(set)\n",
    "    for a, b in pairs:\n",
    "        graph[a].add(b)\n",
    "        graph[b].add(a)\n",
    "    visited = set()\n",
    "    groups = []\n",
    "    def dfs(node, group):\n",
    "        visited.add(node)\n",
    "        group.append(node)\n",
    "        for neighbor in graph[node]:\n",
    "            if neighbor not in visited:\n",
    "                dfs(neighbor, group)\n",
    "    for number in graph:\n",
    "        if number not in visited:\n",
    "            group = []\n",
    "            dfs(number, group)\n",
    "            groups.append(tuple(group))\n",
    "    return groups\n",
    "output_groups = find_groups(interactions)\n",
    "\n",
    "for g in tqdm(output_groups):\n",
    "    grpid=min(g)\n",
    "    for i in g:\n",
    "        if i == grpid:\n",
    "            continue\n",
    "        segmented_frames_arr[segmented_frames_arr == i] = grpid\n",
    "        object_tube_box[grpid]['frame']['start'] = min(object_tube_box[grpid]['frame']['start'], object_tube_box[i]['frame']['start'])\n",
    "        object_tube_box[grpid]['frame']['end'] = max(object_tube_box[grpid]['frame']['end'], object_tube_box[i]['frame']['end'])\n",
    "        object_tube_box[grpid]['x']['start'] = min(object_tube_box[grpid]['x']['start'], object_tube_box[i]['x']['start'])\n",
    "        object_tube_box[grpid]['x']['end'] = max(object_tube_box[grpid]['x']['end'], object_tube_box[i]['x']['end'])\n",
    "        object_tube_box[grpid]['y']['start'] = min(object_tube_box[grpid]['y']['start'], object_tube_box[i]['y']['start'])\n",
    "        object_tube_box[grpid]['y']['end'] = max(object_tube_box[grpid]['y']['end'], object_tube_box[i]['y']['end'])\n",
    "        if i in timestamp_dict:\n",
    "            timestamp_dict[grpid] += timestamp_dict[i]\n",
    "            del timestamp_dict[i]\n",
    "        if i in object_tube_box: del object_tube_box[i]\n",
    "        if i in object_ids: object_ids.remove(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define object tubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_tubes = defaultdict(lambda: dict())\n",
    "object_ids_blacklist = list()\n",
    "itera = tqdm(object_ids, desc = \"Object Tube Initialization\")\n",
    "for oid in itera:  # oid: object id\n",
    "    impure_voxel = segmented_frames_arr[\n",
    "        object_tube_box[oid]['frame']['start']:object_tube_box[oid]['frame']['end']+1,\n",
    "        object_tube_box[oid]['y']['start']:object_tube_box[oid]['y']['end']+1,\n",
    "        object_tube_box[oid]['x']['start']:object_tube_box[oid]['x']['end']+1\n",
    "    ]\n",
    "    if min(impure_voxel.shape)<2:\n",
    "        object_ids_blacklist.append(oid)\n",
    "        continue\n",
    "    object_tubes[oid]['voxel'] = (impure_voxel == oid).astype(np.int8)\n",
    "    object_tubes[oid]['start_frame'] = object_tube_box[oid]['frame']['start']\n",
    "    object_tubes[oid]['start_coord'] = (object_tube_box[oid]['y']['start'], object_tube_box[oid]['x']['start'])\n",
    "    object_tubes[oid]['2d_dim'] = impure_voxel.shape[1:3]\n",
    "    object_tubes[oid]['length'] = impure_voxel.shape[0]\n",
    "\n",
    "    vox = object_tubes[oid]['voxel']#.copy()\n",
    "    vox_filled = set([i for i in range(object_tubes[oid]['length']) if vox[i].sum()!=0])\n",
    "    vox_gpu = tf.convert_to_tensor(vox, dtype=tf.float32)\n",
    "    itera.set_postfix_str(f\"{object_tubes[oid]['length']}\")\n",
    "    if object_tubes[oid]['length']<=video_info.fps*15:\n",
    "        for i in range(object_tubes[oid]['length']):\n",
    "            if i in vox_filled: continue\n",
    "            mul_arr=[(1/abs(i-j) if i!=j else 1) for j in range(object_tubes[oid]['length'])] \n",
    "            mul_arr = tf.convert_to_tensor(mul_arr)\n",
    "            weighted_vox = tf.multiply(vox_gpu, tf.reshape(mul_arr, (object_tubes[oid]['length'], 1, 1)))\n",
    "            layer = weighted_vox.cpu().numpy()\n",
    "            del weighted_vox, mul_arr\n",
    "            layer = np.sum(layer, axis=0)\n",
    "            vox[i]=(layer >= layer[layer != 0].mean()).astype(int)\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    tf.keras.backend.clear_session()\n",
    "    garbage_collect()\n",
    "    padded_voxel = np.pad(vox, pad_width=1, mode='constant', constant_values=0)\n",
    "    verts, faces = None, None\n",
    "    try:\n",
    "        verts, faces, _, _ = marching_cubes(padded_voxel, level=0.5)\n",
    "    except:\n",
    "        object_ids_blacklist.append(oid)\n",
    "        continue\n",
    "    mesh = trimesh.Trimesh(vertices=verts, faces=faces)\n",
    "    mesh.vertices -= (1, 1, 1)\n",
    "    fc = ((object_tubes[oid]['length']//video_info.fps)+1)*16+40\n",
    "    object_tubes[oid]['mesh'] = mesh.simplify_quadric_decimation(face_count=fc) if len(mesh.faces)>fc else mesh\n",
    "    object_tubes[oid]['mesh'].apply_translation((-1*min([i[0] for i in object_tubes[oid]['mesh'].vertices]), object_tubes[oid]['start_coord'][0], object_tubes[oid]['start_coord'][1]))\n",
    "    object_tubes[oid]['mesh'].faces = object_tubes[oid]['mesh'].faces[:, ::-1]\n",
    "\n",
    "for oid in object_ids_blacklist: object_ids.remove(oid)\n",
    "for oid in object_ids_blacklist: \n",
    "    if oid in object_tubes:\n",
    "        del object_tubes[oid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Annealing for Object Tube Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryTree:\n",
    "    def __init__(self):\n",
    "        self.nodes = dict()\n",
    "        self.id_count = 1\n",
    "        self.leafs = set()\n",
    "        self.nodes[0] = Node(self, 0, None, 0)\n",
    "        self.leafs.add(0)\n",
    "        self.max_frame = None\n",
    "        self.max_frame_oid = None\n",
    "        \n",
    "    def addNode(self):\n",
    "        pnode = min(self.leafs, key=lambda x: self.nodes[x].level)\n",
    "        for _ in range(2):\n",
    "            id = self.id_count\n",
    "            self.id_count += 1        \n",
    "            self.nodes[id] = Node(self, id, pnode, self.nodes[pnode].level+1)\n",
    "            self.nodes[pnode].cnodes.append(id)\n",
    "            self.leafs.add(id)\n",
    "        self.leafs.remove(pnode)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, tree: BinaryTree, id: int, pnode: int, level: int):\n",
    "        self.tree = tree\n",
    "        self.id = id\n",
    "        self.pnode = pnode\n",
    "        self.level = level\n",
    "        self.cnodes = list()\n",
    "        self.mesh = None\n",
    "\n",
    "    def initMesh(self):\n",
    "        if self.id not in self.tree.leafs:\n",
    "            for c in self.cnodes:\n",
    "                self.tree.nodes[c].initMesh()\n",
    "            else:\n",
    "                self.mesh = trimesh.boolean.union([self.tree.nodes[i].mesh for i in self.cnodes], check_volume=False)\n",
    "\n",
    "    def _reconstructMesh_(self):\n",
    "        self.mesh = trimesh.boolean.union([self.tree.nodes[i].mesh for i in self.cnodes], check_volume=False)\n",
    "        if self.pnode!=None:\n",
    "            self.tree.nodes[self.pnode]._reconstructMesh_()\n",
    "\n",
    "    def moveMesh(self, shift):\n",
    "        self.mesh.apply_translation((shift, 0, 0))\n",
    "        self.tree.nodes[self.pnode]._reconstructMesh_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = BinaryTree()\n",
    "for _ in range(len(object_tubes)-1):\n",
    "    state.addNode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oid_in_state = dict()\n",
    "for oid, nid in tqdm(zip(object_tubes, state.leafs), desc=\"Initializing workspace variable (Part 1/2)\"):\n",
    "    oid_in_state[oid]=nid\n",
    "    state.nodes[nid].mesh = object_tubes[oid]['mesh'].copy()\n",
    "    state.nodes[nid].mesh.apply_translation((object_tubes[oid]['start_frame'], 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.nodes[0].initMesh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_VOLUME = state.nodes[0].mesh.volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "state.max_frame = float('-inf')\n",
    "state.max_frame_oid = None\n",
    "for oid in tqdm(object_tubes, desc = \"Finding current duration\"):\n",
    "    for i in state.nodes[oid_in_state[oid]].mesh.vertices:\n",
    "        if i[0] > state.max_frame:\n",
    "            state.max_frame = i[0]\n",
    "            state.max_frame_oid = oid\n",
    "\n",
    "def fn_fitness(state):\n",
    "    volume = state.nodes[0].mesh.volume\n",
    "    return state.max_frame, volume, 0.85*(video_info.total_frames/state.max_frame)+0.15*(volume/TOTAL_VOLUME)\n",
    "\n",
    "best_duration, best_union_volume, best_fitness = fn_fitness(state)\n",
    "current_duration, current_union_volume, current_fitness = best_duration, best_union_volume, best_fitness\n",
    "best_state = copy.deepcopy(state)\n",
    "temperature = 5000\n",
    "oid = random.choice(list(object_tubes.keys()))\n",
    "print(\"Iter\\tCR\\tUoS\\tTimestamp\")\n",
    "for i in range(2010):\n",
    "    shift = random.randint(int(-1*min([i[0] for i in state.nodes[oid_in_state[oid]].mesh.vertices])), int(state.max_frame - max([i[0] for i in state.nodes[oid_in_state[oid]].mesh.vertices])))\n",
    "    state.nodes[oid_in_state[oid]].moveMesh(shift)    \n",
    "    end_frame = max([i[0] for i in state.nodes[oid_in_state[oid]].mesh.vertices])\n",
    "    if end_frame > state.max_frame:\n",
    "        state.max_frame = end_frame\n",
    "        state.max_frame_oid = oid\n",
    "    elif oid == state.max_frame_oid:\n",
    "        state.max_frame = float('-inf')\n",
    "        state.max_frame_oid = None\n",
    "        for oid2 in object_tubes:\n",
    "            for j in state.nodes[oid_in_state[oid2]].mesh.vertices:\n",
    "                if j[0] > state.max_frame:\n",
    "                    state.max_frame = j[0]\n",
    "                    state.max_frame_oid = oid2\n",
    "    duration, union_volume, fitness = fn_fitness(state)\n",
    "    delta_fitness = fitness - current_fitness \n",
    "    if delta_fitness > 0:\n",
    "        current_duration = duration\n",
    "        current_union_volume = union_volume\n",
    "        current_fitness = fitness\n",
    "    else:\n",
    "        acceptance_probability = np.exp(delta_fitness / temperature)\n",
    "        if random.random() < acceptance_probability:\n",
    "            current_duration = duration\n",
    "            current_union_volume = union_volume\n",
    "            current_fitness = fitness\n",
    "        else:\n",
    "            state.nodes[oid_in_state[oid]].moveMesh(-1*shift)\n",
    "    if current_fitness > best_fitness:\n",
    "        del best_state\n",
    "        best_state = copy.deepcopy(state)\n",
    "        best_fitness = current_fitness\n",
    "        best_duration = current_duration\n",
    "        best_union_volume = current_union_volume\n",
    "    cooling_rate = 0.85\n",
    "    temperature *= cooling_rate\n",
    "    if i%10==0:\n",
    "        print(f\"{i:04}\\t{(video_info.total_frames/best_duration):1.16f}\\t{(best_union_volume/TOTAL_VOLUME):1.16f}\\t{time.time()}\")\n",
    "    oid = random.choice([state.max_frame_oid, random.choice(list(object_tubes.keys()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_frames = dict()\n",
    "for oid in object_tubes:\n",
    "    start_frames[oid] = min([i[0] for i in best_state.nodes[oid_in_state[oid]].mesh.vertices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synopsis Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_pad_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Could not open video file.\")\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        height, width, channels = frame.shape\n",
    "        pad_height = (32 - height % 32) % 32\n",
    "        pad_width = (32 - width % 32) % 32\n",
    "        top_pad = pad_height // 2\n",
    "        bottom_pad = pad_height - top_pad\n",
    "        left_pad = pad_width // 2\n",
    "        right_pad = pad_width - left_pad\n",
    "        padded_frame = np.pad(\n",
    "            frame, \n",
    "            ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)),\n",
    "            mode='constant', \n",
    "            constant_values=0\n",
    "        )\n",
    "        frames.append(padded_frame)\n",
    "        frame_count += 1\n",
    "    video_array = np.array(frames)\n",
    "    cap.release()\n",
    "    return video_array\n",
    "\n",
    "def extract_background(input_video_array, segmented_frames_arr):\n",
    "    f, h, w = segmented_frames_arr.shape\n",
    "    mask = tf.cast(segmented_frames_arr == 0, tf.float32)\n",
    "    sum_pixels = np.zeros((h, w, 3))\n",
    "    count_valid_frames = np.zeros((h, w))\n",
    "    for frame_idx in range(f):\n",
    "        valid_pixels = mask[frame_idx].numpy()\n",
    "        sum_pixels += input_video_array[frame_idx] * valid_pixels[:, :, np.newaxis]    \n",
    "        count_valid_frames += valid_pixels\n",
    "    epsilon = 1e-8\n",
    "    average_pixels = sum_pixels / (count_valid_frames[:, :, np.newaxis] + epsilon)\n",
    "    average_pixels_normalized = np.clip(average_pixels, 0, 255).astype(np.uint8)\n",
    "    return average_pixels_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsis_video_array = np.zeros((int(best_duration)+3*video_info.fps, segmented_frames_arr.shape[1], segmented_frames_arr.shape[2], 3), dtype=np.uint8)\n",
    "input_video_array = load_and_pad_video(INPUT_VIDEO_PATH)\n",
    "\n",
    "background_frame = extract_background(input_video_array, segmented_frames_arr)\n",
    "synopsis_video_array[:] = background_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for oid in tqdm(start_frames, desc = \"Placing objects on synopsis video\"):\n",
    "    obj_tube_sub_array = object_tubes[oid]['voxel']    \n",
    "    start_coord = object_tubes[oid]['start_coord']\n",
    "    \n",
    "    object_tube_mask = (obj_tube_sub_array != 0).astype(np.uint8)\n",
    "    object_tube_mask = np.repeat(object_tube_mask[:, :, :, np.newaxis], 3, axis=3)\n",
    "    \n",
    "    object_tube_mask_neg = (obj_tube_sub_array == 0).astype(np.uint8)    \n",
    "    object_tube_mask_neg = np.repeat(object_tube_mask_neg[:, :, :, np.newaxis], 3, axis=3)\n",
    "    \n",
    "    corner1 = (int(start_frames[oid]), start_coord[0], start_coord[1])\n",
    "    corner2 = tuple(corner1[i] + object_tube_mask_neg.shape[i] for i in range(3))\n",
    "    f1 = object_tubes[oid]['start_frame']\n",
    "    f2 = f1 + object_tubes[oid]['length']\n",
    "    synopsis_video_array[corner1[0]:corner2[0], corner1[1]:corner2[1], corner1[2]:corner2[2]] *= object_tube_mask_neg\n",
    "    synopsis_video_array[corner1[0]:corner2[0], corner1[1]:corner2[1], corner1[2]:corner2[2]] += object_tube_mask * input_video_array[f1:f2, corner1[1]:corner2[1], corner1[2]:corner2[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_to_frame(video_array, frame_number, text, desired_width, coordinates):\n",
    "    frame = video_array[frame_number]\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.5\n",
    "    thickness = 1\n",
    "    (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "    x, y = coordinates\n",
    "    y += text_height\n",
    "    font_color = (255, 255, 255)\n",
    "    cv2.putText(frame, text, (int(x), y), font, font_scale, font_color, thickness)\n",
    "for oid in tqdm(start_frames, desc=\"Adding timestamp\"):\n",
    "    change = int(start_frames[oid]) - object_tubes[oid]['start_frame']\n",
    "    for ts in timestamp_dict[oid]:\n",
    "        add_text_to_frame(synopsis_video_array, ts[0]+change, f\"{(ts[0] // video_info.fps) // 60:02}:{(ts[0] // video_info.fps) % 60:02}\", 2, (ts[1], ts[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = ImageSequenceClip(list(synopsis_video_array[..., ::-1]), fps=video_info.fps)\n",
    "clip.write_videofile('output.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display system status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!free -h\n",
    "!lscpu"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
